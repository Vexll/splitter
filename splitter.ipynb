{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-18T18:45:25.083006Z",
     "start_time": "2024-10-18T18:45:25.079614Z"
    }
   },
   "source": [
    "import os\n",
    "import shutil"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "getting the correct name for directory",
   "id": "71b71dfbfe6163a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T18:45:25.381522Z",
     "start_time": "2024-10-18T18:45:25.375376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_dir_name(dir_name):\n",
    "    if not os.path.isdir(dir_name):\n",
    "        return dir_name\n",
    "\n",
    "    path_contents = os.listdir(f\"{os.getcwd()}\")\n",
    "    target_dirs = [_ for _ in path_contents if dir_name in _]\n",
    "    target_dirs.remove('prepared_dataset')\n",
    "    if len(target_dirs) == 1:\n",
    "        return f'{dir_name}_01'\n",
    "\n",
    "    last_two_elements = target_dirs[-1][-2:]\n",
    "    dir_num = int(last_two_elements) + 1\n",
    "\n",
    "    return f'{dir_name}_{dir_num:02}'"
   ],
   "id": "278f3f84369b8f96",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "check if dataset could be split",
   "id": "467e0b631d9f19cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T18:45:25.991677Z",
     "start_time": "2024-10-18T18:45:25.986684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_dataset_valid(images_path, labels_path):\n",
    "    images_numbers, labels_numbers = len(os.listdir(images_path)), len(os.listdir(labels_path))\n",
    "    if images_numbers == labels_numbers:\n",
    "        return True\n",
    "    return False"
   ],
   "id": "dc798a10685f11b3",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "create the collection of target directory",
   "id": "3da95ca2d9792bbf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T18:45:26.685316Z",
     "start_time": "2024-10-18T18:45:26.679539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dir_collection(src_path):\n",
    "    os.mkdir(src_path)\n",
    "    images_path, labels_path = f\"{src_path}/images\", f\"{src_path}/labels\"\n",
    "    os.mkdir(images_path)\n",
    "    os.mkdir(labels_path)"
   ],
   "id": "facb664de6e263fa",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "checks if directory exist",
   "id": "9df4a9287d1ce5bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T18:45:27.436837Z",
     "start_time": "2024-10-18T18:45:27.432921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_dir_exist(path):\n",
    "    return os.path.isdir(path)"
   ],
   "id": "5d83521ba5f8acf9",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "decompose directory and the prepared dataset to be split",
   "id": "fe869e550668ee7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T18:45:28.154208Z",
     "start_time": "2024-10-18T18:45:28.147708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def decompose(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise Exception(\"the given path is incorrect!\")\n",
    "\n",
    "    target_dir = ['train', 'valid', 'test']\n",
    "    \n",
    "    # check if desire directory exist\n",
    "    pd_dir = f\"{os.getcwd()}/prepared_dataset\"\n",
    "    dst_images_dir = f\"{pd_dir}/images\"\n",
    "    dst_labels_dir = f\"{pd_dir}/labels\"\n",
    "    if is_dir_exist(pd_dir):\n",
    "        raise Exception(\"you must delete 'prepared_dataset folder'!\")\n",
    "\n",
    "    os.mkdir(pd_dir)\n",
    "    os.mkdir(dst_images_dir)\n",
    "    os.mkdir(dst_labels_dir)\n",
    "\n",
    "    for dir in target_dir:\n",
    "        src_dir = f\"{path}/{dir}\"\n",
    "        images_src = f\"{src_dir}/images\"\n",
    "        labels_src = f\"{src_dir}/labels\"\n",
    "        images_list = os.listdir(images_src)\n",
    "        labels_list = os.listdir(labels_src)\n",
    "        for img in images_list:\n",
    "            shutil.copy2(f\"{images_src}/{img}\", dst_images_dir)\n",
    "\n",
    "        for label in labels_list:\n",
    "            shutil.copy2(f\"{labels_src}/{label}\", dst_labels_dir)"
   ],
   "id": "d5af02441f73cea4",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Computer Vision dataset split",
   "id": "f763d5f813645a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T18:45:29.089138Z",
     "start_time": "2024-10-18T18:45:29.078166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cv_split(raw_dataset_path: str, train_s: int, valid_s: int, test_s: int):\n",
    "    check_size = train_s + valid_s + test_s\n",
    "    if check_size != 100:\n",
    "        raise Exception('make sure that your split data size is incorrect!')\n",
    "\n",
    "    images_path = os.path.join(raw_dataset_path, 'images')\n",
    "    labels_path = os.path.join(raw_dataset_path, 'labels')\n",
    "\n",
    "    if not is_dataset_valid(images_path, labels_path):\n",
    "        raise Exception('make sure that number of images equal to labels!')\n",
    "\n",
    "    files = (os.listdir(images_path), os.listdir(labels_path))\n",
    "    dataset_dir = get_dir_name('dataset')\n",
    "    os.mkdir(dataset_dir)\n",
    "\n",
    "    #number of elements\n",
    "    num_elements = get_elements(files[0], train_s, valid_s, test_s)\n",
    "\n",
    "    #creating train dir\n",
    "    train_path = f'{dataset_dir}/train'\n",
    "    create_dir_collection(train_path)\n",
    "    setup_train(files, raw_dataset_path, train_path, num_elements['train'])\n",
    "\n",
    "    #creating valid dir\n",
    "    valid_path = f'{dataset_dir}/valid'\n",
    "    create_dir_collection(valid_path)\n",
    "    valid_index = num_elements['train']\n",
    "    setup_test_or_valid(files, raw_dataset_path, valid_path, num_elements['valid'], valid_index)\n",
    "\n",
    "    #creating temp dir\n",
    "    test_path = f'{dataset_dir}/test'\n",
    "    create_dir_collection(test_path)\n",
    "    test_index = num_elements['train'] + num_elements['valid']\n",
    "    setup_test_or_valid(files, raw_dataset_path, test_path, num_elements['temp'], test_index)\n",
    "\n",
    "\n",
    "def get_elements(f_list, train_s: int, valid_s: int, test_s: int):\n",
    "    train_e = len(f_list) * train_s / 100\n",
    "    if train_e % 1 != 0:\n",
    "        train_e = int(train_e) + 1\n",
    "        valid_e = len(f_list) * valid_s // 100\n",
    "        test_e = len(f_list) * test_s // 100\n",
    "    else:\n",
    "        valid_e = len(f_list) * valid_s / 100\n",
    "        if valid_e % 1 != 0:\n",
    "            valid_e = int(valid_e) + 1\n",
    "            test_e = len(f_list) * test_s // 100\n",
    "        else:\n",
    "            test_e = len(f_list) * test_s / 100\n",
    "            if test_e % 1 != 0:\n",
    "                test_e = int(test_e) + 1\n",
    "\n",
    "    elements = {\n",
    "        'train': int(train_e),\n",
    "        'valid': int(valid_e),\n",
    "        'temp': int(test_e)\n",
    "    }\n",
    "    return elements\n",
    "\n",
    "\n",
    "def setup_train(files, src_path, dst_path, num_elements):\n",
    "    src_img_path, dst_img_path = f\"{src_path}/images\", f\"{dst_path}/images\"\n",
    "    src_label_path, dst_label_path = f\"{src_path}/labels\", f\"{dst_path}/labels\"\n",
    "    for image_name, label_name in zip(files[0][:num_elements], files[1][:num_elements]):\n",
    "        image_to_copy = os.path.join(src_img_path, image_name)\n",
    "        shutil.copy2(image_to_copy, dst_img_path)  #copy the image \n",
    "        label_to_copy = os.path.join(src_label_path, label_name)\n",
    "        shutil.copy2(label_to_copy, dst_label_path)  #copy the label\n",
    "\n",
    "\n",
    "def setup_test_or_valid(files, src_path, dst_path, num_elements, start_index):\n",
    "    src_img_path, dst_img_path = f\"{src_path}/images\", f\"{dst_path}/images\"\n",
    "    src_label_path, dst_label_path = f\"{src_path}/labels\", f\"{dst_path}/labels\"\n",
    "\n",
    "    for image_name, label_name in zip(files[0][start_index: start_index + num_elements],\n",
    "                                      files[1][start_index: start_index + num_elements]):\n",
    "        image_to_copy = os.path.join(src_img_path, image_name)\n",
    "        shutil.copy2(image_to_copy, dst_img_path)  #copy the image \n",
    "        label_to_copy = os.path.join(src_label_path, label_name)\n",
    "        shutil.copy2(label_to_copy, dst_label_path)  #copy the label\n"
   ],
   "id": "2758b50f0563a89a",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Firstly, decompose your dataset by using the decompose function. write down in 'dataset_path' variable the path of your dataset.",
   "id": "b6fd8bc684b8c664"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Make sure that there is no folder inside in your current path called 'dataset'",
   "id": "6a9df63ba5850b9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T18:47:36.388923Z",
     "start_time": "2024-10-18T18:47:36.226485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_path = 'clash_roboflow'\n",
    "decompose(dataset_path)"
   ],
   "id": "eaea5b29ab58d512",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In each variable write down the needed inputs",
   "id": "92858152864ed86f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T18:49:48.612975Z",
     "start_time": "2024-10-18T18:49:48.446509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_size = 80\n",
    "valid_size = 10\n",
    "test_size = 10\n",
    "cv_split('prepared_dataset', train_size, valid_size, test_size)"
   ],
   "id": "3d2c3a2f319e88c1",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "30fd7f369b029109"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
